/*!
 ******************************************************************************
 *
 * \file
 *
 * \brief   Header file containing RAJA reduction templates for CUDA execution.
 *
 *          These methods should work on any platform that supports
 *          CUDA devices.
 *
 ******************************************************************************
 */

//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//
// Copyright (c) 2016-21, Lawrence Livermore National Security, LLC
// and RAJA project contributors. See the RAJA/COPYRIGHT file for details.
//
// SPDX-License-Identifier: (BSD-3-Clause)
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//

#ifndef RAJA_util_GPUReducerTally_HPP
#define RAJA_util_GPUReducerTally_HPP

#include "RAJA/config.hpp"

#include <map>
#include <type_traits>

#include "RAJA/util/macros.hpp"
#include "RAJA/util/Allocator.hpp"
#include "RAJA/util/mutex.hpp"
#include "RAJA/util/types.hpp"

#if defined(RAJA_ENABLE_CUDA)
#include "RAJA/policy/cuda/MemUtils_CUDA.hpp"
#include "RAJA/policy/cuda/raja_cudaerrchk.hpp"
#endif

#if defined(RAJA_ENABLE_HIP)
#include "RAJA/policy/hip/MemUtils_HIP.hpp"
#include "RAJA/policy/hip/raja_hiperrchk.hpp"
#endif

// TODO: Remove this once omp::mutex is removed
#if defined(RAJA_ENABLE_OPENMP) && !defined(_OPENMP)
#error RAJA configured with ENABLE_OPENMP, but OpenMP not supported by current compiler
#endif

namespace RAJA
{

namespace detail
{

template < typename Resource >
struct ResourceInfo;

#if defined(RAJA_ENABLE_CUDA)

template < >
struct ResourceInfo<resources::Cuda>
{
  static Allocator& get_pinned_allocator() {
    return cuda::get_pinned_allocator();
  }
  static Allocator& get_device_allocator() {
    return cuda::get_device_allocator();
  }
  static Allocator& get_device_zeroed_allocator() {
    return cuda::get_device_zeroed_allocator();
  }
  using identifier = cudaStream_t;
  static cudaStream_t get_identifier(resources::Cuda& r)
  {
    return r.get_stream();
  }
  static void synchronize(cudaStream_t s)
  {
    cuda::synchronize(s);
  }
  static bool get_tl_launch_info(size_t& num_teams, cudaStream_t& id)
  {
    RAJA::cuda::detail::LaunchInfo* tl_launch_info = cuda::get_tl_launch_info();
    if (tl_launch_info != nullptr) {
      num_teams = tl_launch_info->gridDim.x *
                  tl_launch_info->gridDim.y *
                  tl_launch_info->gridDim.z ;
      id = tl_launch_info->stream;
      return true;
    } else {
      return false;
    }
  }
};

#endif


#if defined(RAJA_ENABLE_HIP)

template < >
struct ResourceInfo<resources::Hip>
{
  static Allocator& get_pinned_allocator() {
    return hip::get_pinned_allocator();
  }
  static Allocator& get_device_allocator() {
    return hip::get_device_allocator();
  }
  static Allocator& get_device_zeroed_allocator() {
    return hip::get_device_zeroed_allocator();
  }
  using identifier = hipStream_t;
  static hipStream_t get_identifier(resources::Hip& r)
  {
    return r.get_stream();
  }
  static void synchronize(hipStream_t s)
  {
    hip::synchronize(s);
  }
  static bool get_tl_launch_info(size_t& num_teams, hipStream_t& id)
  {
    RAJA::hip::detail::LaunchInfo* tl_launch_info = hip::get_tl_launch_info();
    if (tl_launch_info != nullptr) {
      num_teams = tl_launch_info->gridDim.x *
                  tl_launch_info->gridDim.y *
                  tl_launch_info->gridDim.z ;
      id = tl_launch_info->stream;
      return true;
    } else {
      return false;
    }
  }
};

#endif


//! Object that manages pinned memory buffers for reduction results
//  use one per reducer object
template <typename T, typename Resource>
class GPUReducerTally
{
  using resource_info = ResourceInfo<Resource>;
  using identifier = typename resource_info::identifier;
public:
  //! Object put in Pinned memory with value and pointer to next ValueNode
  struct ValueNode {
    ValueNode* next;
    T value;
  };

  struct MemoryNode {
    bool in_use;
    void* device_memory;
    unsigned int* device_count_ptr;
  };

  //! Object per id to keep track of pinned memory nodes
  struct ResourceNode {
    ResourceNode* next;
    identifier id;
    ValueNode* node_list;
    std::multimap<size_t, MemoryNode> size_to_memory_map;
  };

  //! Iterator over streams used by reducer
  class StreamIterator
  {
  public:
    StreamIterator() = delete;

    StreamIterator(ResourceNode* rn) : m_rn(rn) {}

    const StreamIterator& operator++()
    {
      m_rn = m_rn->next;
      return *this;
    }

    StreamIterator operator++(int)
    {
      StreamIterator ret = *this;
      this->operator++();
      return ret;
    }

    identifier& operator*() { return m_rn->id; }

    bool operator==(const StreamIterator& rhs) const
    {
      return m_rn == rhs.m_rn;
    }

    bool operator!=(const StreamIterator& rhs) const
    {
      return !this->operator==(rhs);
    }

  private:
    ResourceNode* m_rn;
  };

  //! Iterator over all values generated by reducer
  class ResourceNodeIterator
  {
  public:
    ResourceNodeIterator() = delete;

    ResourceNodeIterator(ResourceNode* rn, ValueNode* n) : m_rn(rn), m_n(n) {}

    const ResourceNodeIterator& operator++()
    {
      if (m_n->next) {
        m_n = m_n->next;
      } else if (m_rn->next) {
        m_rn = m_rn->next;
        m_n = m_rn->node_list;
      } else {
        m_rn = nullptr;
        m_n = nullptr;
      }
      return *this;
    }

    ResourceNodeIterator operator++(int)
    {
      ResourceNodeIterator ret = *this;
      this->operator++();
      return ret;
    }

    T& operator*() { return m_n->value; }

    bool operator==(const ResourceNodeIterator& rhs) const
    {
      return m_n == rhs.m_n;
    }

    bool operator!=(const ResourceNodeIterator& rhs) const
    {
      return !this->operator==(rhs);
    }

  private:
    ResourceNode* m_rn;
    ValueNode* m_n;
  };

  GPUReducerTally() : stream_list(nullptr) {}

  GPUReducerTally(const GPUReducerTally&) = delete;

  //! get begin iterator over streams
  StreamIterator streamBegin() { return {stream_list}; }

  //! get end iterator over streams
  StreamIterator streamEnd() { return {nullptr}; }

  //! get begin iterator over values
  ResourceNodeIterator begin()
  {
    return {stream_list, stream_list ? stream_list->node_list : nullptr};
  }

  //! get end iterator over values
  ResourceNodeIterator end() { return {nullptr, nullptr}; }

  //! get new value and pointers based on arguments
  MemoryNode* new_value(size_t num_teams,
                        identifier id,
                        T*& value_ptr,
                        T*& device_atomic_ptr,
                        unsigned int*& device_count_ptr)
  {
    void* device_memory = nullptr;
    const size_t device_memory_size = sizeof(T);

    MemoryNode* n = new_value_impl(id,
                                   value_ptr,
                                   device_memory, device_memory_size,
                                   device_count_ptr);

    device_atomic_ptr = static_cast<T*>(device_memory);

    return n;
  }

  //! get new value and pointers based on thread local launch info
  MemoryNode* new_value_tl(T*& value_ptr,
                           T*& device_atomic_ptr,
                           unsigned int*& device_count_ptr)
  {
    size_t num_teams;
    identifier id;
    if (resource_info::get_tl_launch_info(num_teams, id)) {

      return new_value(num_teams,
                       id,
                       value_ptr,
                       device_atomic_ptr,
                       device_count_ptr);
    } else {
      return nullptr;
    }
  }

  //! get new value and pointers based on arguments
  MemoryNode* new_value(size_t num_teams,
                        identifier id,
                        T*& value_ptr,
                        SoAPtr<T>& device_soa_ptr,
                        unsigned int*& device_count_ptr)
  {
    void* device_memory = nullptr;
    const size_t device_memory_size = device_soa_ptr.allocationSize(num_teams);

    MemoryNode* n = new_value_impl(id,
                                   value_ptr,
                                   device_memory, device_memory_size,
                                   device_count_ptr);

    device_soa_ptr.setMemory(num_teams, device_memory);

    return n;
  }

  //! get new value and pointers based on thread local launch info
  MemoryNode* new_value_tl(T*& value_ptr,
                           SoAPtr<T>& device_soa_ptr,
                           unsigned int*& device_count_ptr)
  {
    size_t num_teams;
    identifier id;
    if (resource_info::get_tl_launch_info(num_teams, id)) {

      return new_value(num_teams,
                       id,
                       value_ptr,
                       device_soa_ptr,
                       device_count_ptr);
    } else {
      return nullptr;
    }
  }

  void reuse_memory(MemoryNode* mn)
  {
    mn->in_use = false;
  }

  //! synchronize all streams used
  void synchronize_streams()
  {
    auto end = streamEnd();
    for (auto s = streamBegin(); s != end; ++s) {
      resource_info::synchronize(*s);
    }
  }

  //! all values used in all streams
  void free_list()
  {
    while (stream_list) {
      ResourceNode* rn = stream_list;
      while (rn->node_list) {
        ValueNode* n = rn->node_list;
        rn->node_list = n->next;
        resource_info::get_pinned_allocator().deallocate(n);
      }
      for (auto& mn : rn->size_to_memory_map) {
        resource_info::get_device_allocator().deallocate(mn.second.device_memory);
        resource_info::get_device_zeroed_allocator().deallocate(mn.second.device_count_ptr);
        if (mn.second.in_use) {
          RAJA_ABORT_OR_THROW("GPUReducerTally: Can't free MemoryNode that is in use");
        }
      }
      stream_list = rn->next;
      delete rn;
    }
  }

  ~GPUReducerTally() { free_list(); }

#if defined(RAJA_ENABLE_OPENMP)
  omp::mutex m_mutex;
#endif

private:
  ResourceNode* stream_list;

  MemoryNode* new_value_impl(identifier id,
                             T*& value_ptr,
                             void*& device_memory, size_t device_memory_size,
                             unsigned int*& device_count_ptr)
  {
#if defined(RAJA_ENABLE_OPENMP)
    lock_guard<omp::mutex> lock(m_mutex);
#endif
    // find ResourceNode for id
    ResourceNode* rn;
    for (rn = stream_list; rn != nullptr; rn = rn->next) {
      if (rn->id == id) break;
    }

    // allocate ResourceNode if not found
    if (!rn) {
      rn = new ResourceNode;
      rn->next = stream_list;
      rn->id = id;
      rn->node_list = nullptr;
      stream_list = rn;
    }

    // allocate ValueNode
    ValueNode* vn = resource_info::get_pinned_allocator().template
        allocate<ValueNode>(1);
    vn->next = rn->node_list;
    rn->node_list = vn;

    // find MemoryNode with enough storage
    MemoryNode* mn = nullptr;
    for (auto mn_i = rn->size_to_memory_map.lower_bound(device_memory_size);
         mn_i != rn->size_to_memory_map.end();
         ++mn_i) {
      if (!mn_i->second.in_use) {
        mn = &mn_i->second;
        break;
      }
    }

    // allocate MemoryNode if not found
    if (mn == nullptr) {
      auto mn_i = rn->size_to_memory_map.emplace(
        device_memory_size,
        MemoryNode{
          false,
          resource_info::get_device_allocator().
              allocate(device_memory_size),
          resource_info::get_device_zeroed_allocator().template
              allocate<unsigned int>(1)
        });
      mn = &mn_i->second;
    }

    // indicate MemoryNode is in use
    mn->in_use = true;

    value_ptr        = &vn->value;
    device_memory    = mn->device_memory;
    device_count_ptr = mn->device_count_ptr;

    return mn;
  }
};

}  // end namespace detail

}  // namespace RAJA

#endif  // closing endif for header file include guard
